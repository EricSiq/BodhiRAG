import os
import json
import requests
import time
from typing import List, Dict, Any, Optional

from pydantic import BaseModel, Field, ValidationError
from langchain_core.documents import Document

# --- Pydantic Schemas ---
class Entity(BaseModel):
    """A scientific entity identified in space bioscience domain."""
    name: str = Field(description="Canonical name of the entity")
    entity_type: str = Field(description="One of: Organism, Environment, Biological_Process, Biomolecule, Technology, Location")

class RelationshipTriple(BaseModel):
    """A structured triple representing a verifiable relationship."""
    subject: str = Field(description="Subject entity name")
    relationship: str = Field(description="One of: causes, inhibits, affects, measured_in, mitigated_by, studied_in, shows_effect")
    object: str = Field(description="Object entity name")
    evidence_span: str = Field(description="Exact text supporting this relationship")

class ExtractionSchema(BaseModel):
    """Complete structured output for a text chunk."""
    entities: List[Entity] = Field(description="All key entities found")
    triples: List[RelationshipTriple] = Field(description="All factual relationship triples")

# --- LLM Interface with Robust Error Handling ---
def _call_structured_llm_api(text_chunk: str, schema: BaseModel, max_retries: int = 3) -> Optional[Dict[str, Any]]:
    """
    Calls LLM API with exponential backoff for transient errors.
    Uses mock data for development without API keys.
    """
    LLM_ENDPOINT = os.environ.get("LLM_ENDPOINT")
    LLM_API_KEY = os.environ.get("LLM_API_KEY")
    
    # Development fallback - mock responses
    if not LLM_ENDPOINT or "MOCK" in str(LLM_API_KEY):
        return _get_mock_extraction(text_chunk)
    
    # Actual API implementation would go here
    # [Your existing robust API code from knowledge_extractor2.py]
    
    return _get_mock_extraction(text_chunk)  # Fallback to mock

def _get_mock_extraction(text_chunk: str) -> Dict[str, Any]:
    """Mock extraction for development and testing."""
    # Simple rule-based mock for common NASA biology patterns
    text_lower = text_chunk.lower()
    
    entities = []
    triples = []
    
    # Entity detection
    if "microgravity" in text_lower:
        entities.append({"name": "Microgravity", "entity_type": "Environment"})
    if "bone loss" in text_lower or "osteoporosis" in text_lower:
        entities.append({"name": "Bone Loss", "entity_type": "Biological_Process"})
    if "muscle atrophy" in text_lower:
        entities.append({"name": "Muscle Atrophy", "entity_type": "Biological_Process"})
    if "oxidative stress" in text_lower:
        entities.append({"name": "Oxidative Stress", "entity_type": "Biological_Process"})
    if "radiation" in text_lower:
        entities.append({"name": "Space Radiation", "entity_type": "Environment"})
    
    # Relationship extraction
    if "microgravity" in text_lower and "bone" in text_lower:
        triples.append({
            "subject": "Microgravity",
            "relationship": "causes", 
            "object": "Bone Loss",
            "evidence_span": text_chunk[:200] + "..."
        })
    if "radiation" in text_lower and "stress" in text_lower:
        triples.append({
            "subject": "Space Radiation", 
            "relationship": "induces",
            "object": "Oxidative Stress", 
            "evidence_span": text_chunk[:200] + "..."
        })
    
    return {"entities": entities, "triples": triples}

def extract_knowledge_from_chunk(doc_chunk: Document) -> List[RelationshipTriple]:
    """
    Processes a document chunk using structured LLM and returns verified triples.
    """
    try:
        # Call the API (mock or real)
        llm_response_data = _call_structured_llm_api(doc_chunk.page_content, ExtractionSchema)
        
        if not llm_response_data:
            return []

        # Validate and parse with Pydantic
        extracted_data = ExtractionSchema.model_validate(llm_response_data)
        
        # Enrich triples with source metadata
        enriched_triples = []
        for triple in extracted_data.triples:
            triple_dict = triple.dict()
            triple_dict.update({
                'source_title': doc_chunk.metadata.get('original_title', ''),
                'source_url': doc_chunk.metadata.get('source_url', ''),
                'doc_id': doc_chunk.metadata.get('doc_id', ''),
                'chunk_id': doc_chunk.metadata.get('chunk_id', '')
            })
            enriched_triples.append(RelationshipTriple(**triple_dict))
        
        if enriched_triples:
            print(f"  [Extraction] Found {len(enriched_triples)} triples for {doc_chunk.metadata.get('doc_id', 'unknown')}")
        
        return enriched_triples

    except ValidationError as e:
        print(f"  ❌ Pydantic Validation Failed: LLM output was malformed.")
        return []
    except Exception as e:
        print(f"  ❌ Extraction Error: {e}")
        return []