{
  "project_name": "BodhiRAG: A Wisdom Graph for Exploring Space Biology Research",
  "challenge_focus": "Build a Space Biology Knowledge Engine (NASA Space Apps Challenge 2025)",
  "target_audience": ["Scientists (Hypotheses)", "Managers (Investment/Gaps)", "Mission Architects (Risk)"],
  "data_scope": {
    "primary_data_source": "607+ NASA Bioscience Publications (PMC URLs)",
    "secondary_data_sources": ["NASA OSDR Data (for linkage)", "NASA Task Book (for funding metadata)"]
  },
  "core_architecture": "Agentic Hybrid RAG (Retrieval-Augmented Generation) with Granite Integration",
  
  "architecture_components": [
    {
      "component": "1. Data Structuring & Ingestion (Granite-Docling Pipeline)",
      "function": "High-fidelity conversion of unstructured PDFs/Images into structured, semantic Markdown for superior RAG quality.",
      "workflow_steps": [
        "**Granite-Docling-258M** for VLM-based document parsing (replaces Docling/Unstructured parsing).",
        "Output to **DocTags** then final **Markdown** to preserve layout, tables, and equations.",
        "Max-Normalized Chunking on structured Markdown (improves RAG context retrieval)."
      ],
      "python_libraries": ["pandas", "docling-core", "ibm-granite (model access)", "spacy", "langchain-core"]
    },
    {
      "component": "2. Knowledge Graph (KG) Creation",
      "function": "Store relationships and logical facts for multi-hop reasoning.",
      "workflow_steps": [
        "LLM-driven NER/RE using **Granite LLM** for superior entity/relation extraction.",
        "Pydantic Schema Validation (ensures data quality and consistency).",
        "KG Population (e.g., in Neo4j)."
      ],
      "python_libraries": ["transformers (for LLM)", "pydantic", "networkx (analysis)", "neo4j (driver/database)"]
    },
    {
      "component": "3. Vector Database (VS) Creation",
      "function": "Store semantic embeddings of text chunks for descriptive similarity search.",
      "workflow_steps": [
        "Embedding Generation using **Granite Embedding Model** (e.g., `granite-embedding-30m-english`).",
        "VS Indexing (chroma/milvus).",
        "Linking vectors to KG node IDs (for Hybrid RAG)."
      ],
      "python_libraries": ["sentence-transformers", "chroma/milvus (database)", "langchain_huggingface (for Granite Embeddings)"]
    },
    {
      "component": "4. Agentic Reasoning Core (LangGraph/Granite Agent)",
      "function": "Dynamically route user queries to the best knowledge source (KG/VS/Tool) and synthesize a grounded answer.",
      "workflow_steps": [
        "Query Intent Analysis (LangChain's **Granite Agent** with ReAct framework).",
        "Dynamic Routing (**LangGraph** orchestration).",
        "LLM Synthesis using a compact, efficient **Granite-4.0 Hybrid Model** (SLM) for fast, cost-effective generation."
      ],
      "python_libraries": ["langgraph", "langchain", "**langchain-ibm** (for Watsonx/Granite connection)", "transformers"]
    }
  ],
  
  "key_features_and_xai": [
    {
      "feature": "Hybrid RAG Chatbot",
      "purpose": "Answers complex queries by combining factual triples (KG) and contextual text (VS).",
      "xai_aspect": "Source Traceability (URL links) and Retrieval Path Log (KG or VS used, generated by the LangGraph agent)."
    },
    {
      "feature": "Interactive Knowledge Map",
      "purpose": "Visualizes relationships; filters by entity type, environment, or process.",
      "python_libraries": ["pyvis", "plotly-dash (integration)"],
      "xai_aspect": "Node Size based on Centrality; Node Color based on Entity Type."
    },
    {
      "feature": "Gaps & Investment Analysis",
      "purpose": "Identifies over/under-researched areas.",
      "python_libraries": ["bertopic", "networkx (scoring)"],
      "xai_aspect": "Augmented Scoring (Centrality + Topic Relevance) to quantify research impact and gaps."
    }
  ],
  
  "deployment_stack": {
    "backend_framework": "Python/FastAPI (for low-latency API endpoints)",
    "dashboard_framework": "Plotly Dash / Streamlit",
    "ui_libraries": "Vis.js / Plotly.js (for rendering graph and UMAP plots)",
    "compute_requirement": "**Optimized for Single-Node/High-End Laptop Deployment**",
    "cost_optimization_strategy": [
      "Use of **Granite-4.0 Hybrid SLMs** (small, hyper-efficient models) over large general-purpose models.",
      "Deployment of models using **vLLM** or **ONNX** with **quantization** (`--dtype float32` for Granite-Docling if needed) to minimize VRAM usage and maximize inference speed on a local GPU.",
      "Offload compute to **IBM watsonx.ai** for the main Granite LLM inference if local capacity is insufficient (using the `langchain-ibm` wrapper)."
    ]
  }
  {
  "demo_scenarios": [
    {
      "scientist_query": "What countermeasures exist for microgravity-induced bone loss?",
      "expected_behavior": "Multi-hop reasoning through KG + evidence from papers"
    },
    {
      "manager_query": "Show me research gaps in radiation protection for Mars missions", 
      "expected_behavior": "Topic modeling + centrality analysis visualization"
    }
  ]
  }
}
